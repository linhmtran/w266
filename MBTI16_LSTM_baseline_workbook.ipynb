{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LSTM MBTI Classification Model\n",
    "\n",
    "First, load libraries and useful functions from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Utils and Helper libraries\n",
    "# import nltk\n",
    "from w266_common import utils, vocabulary\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Specifications for Baseline LSTM for MBTI\n",
    "\n",
    "In this baseline, the task is to predict the MBTI type (16 types) given a text string. We will model after the A3 assignment, with Architecture and Parameters defined below.\n",
    "\n",
    "### Pre-Processing:\n",
    "* Minimial pre-processing, only separating punctuation from text and lower-case all text\n",
    "* Assigning words to numerical indices based on a fixed Vocab size, defined by word fre-quency in training set\n",
    "\n",
    "### Architecture:\n",
    "* Encoder: 2 layer LSTM\n",
    "* Decoder: Affine on the LSTM output with Softmax on logits\n",
    "* Classification: 16 MBTI types\n",
    "\n",
    "### Parameters\n",
    "* Batch Size: 500\n",
    "* Text length: 250\n",
    "* Vocabulary size (V): ~330K - full corpus\n",
    "* Embedding Size: 300\n",
    "* Hidden Dimensions: 300\n",
    "\n",
    "### Training:\n",
    "* Epochs = 25\n",
    "* 80% train, 20% test\n",
    "* Loss: Cross Entropy with Adam initialization\n",
    "* Optimizers: Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus & Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('./mbti_1.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean and tokenize sentence [\"Hello world.\"] into list of words [\"hello world\"]\n",
    "def clean(sentence):\n",
    "    ignore_words = []\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() #nltk.word_tokenize(sentence)\n",
    "#     words = sentence.split() #nltk.word_tokenize(sentence)\n",
    "    words_cleaned = sentence.lower()\n",
    "    return words_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(422845, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'http://www.youtube.com/watch?v=qsxhcwe3krw</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://41.media.tumblr.com/tumblr_lfouy03pma1q...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what has been the most life-changing experienc...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.youtube.com/watch?v=vxzeywwrdw8   h...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  type  user\n",
       "0        'http://www.youtube.com/watch?v=qsxhcwe3krw  INFJ     0\n",
       "1  http://41.media.tumblr.com/tumblr_lfouy03pma1q...  INFJ     0\n",
       "2  enfp and intj moments  https://www.youtube.com...  INFJ     0\n",
       "3  what has been the most life-changing experienc...  INFJ     0\n",
       "4  http://www.youtube.com/watch?v=vxzeywwrdw8   h...  INFJ     0"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split posts per users into separate sentences\n",
    "post = []\n",
    "utype = []\n",
    "user = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    posts = row['posts'].split('|||')\n",
    "    posts_clean = []\n",
    "    for sentence in posts:\n",
    "        posts_clean.append(clean(sentence))\n",
    "    post.extend(posts_clean)\n",
    "#     post.extend(posts)\n",
    "    utype.extend([row['type'] for i in range(len(posts))])\n",
    "    user.extend([index for i in range(len(posts))])\n",
    "    \n",
    "short_posts = pd.DataFrame({\"user\": user,\"type\": utype,\"post\": post})\n",
    "print(short_posts.shape)\n",
    "short_posts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBIT posts ['https://www.youtube.com/watch?v=bxvkaah2d7m'\n",
      " 'isfjs and infps can balance each other really well, i think, if they learn to communicate - sjs choosing words (and tone of voice!) carefully, and nps learning to take things less personally. your sj...'\n",
      " \"i'd seek recognition. not fame.\"\n",
      " \"to be honest, maybe you are giving out vibes that you are not self-assured. because if you think about it, a bully is going to go for someone they don't think will fight back, someone who is weak (i...\"\n",
      " \"probably. any thinking, really. personally, i prefer the ax-b-c-dy function stack compared to grant's. it allows for the parts where grant gets it right while still being consistent with jung, i.e.,...\"]\n",
      "\n",
      "MBTI Labels:  ['INTP' 'INFP' 'INTP' 'ENFP' 'INTJ']\n"
     ]
    }
   ],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "post_train, post_test, label_train, label_test = train_test_split(np.array(short_posts['post']), \n",
    "                                                    np.array(short_posts['type']), \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=88)\n",
    "\n",
    "print(\"MBIT posts\", post_train[:5])\n",
    "print('')\n",
    "print(\"MBTI Labels: \",label_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333007"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary (V size is defaulted to full text) for train corpus\n",
    "vocab_mbti = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in post_train))\n",
    "vocab_mbti.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202, 147565, 317206, 159348]\n",
      "['a', 'what', 'and', 'the']\n"
     ]
    }
   ],
   "source": [
    "print (vocab_mbti.words_to_ids(['a','what','and','the']))\n",
    "print (vocab_mbti.ids_to_words([202, 147565, 317206, 159348])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenize and canonicalize train and test sets\n",
    "x_train = []\n",
    "for post in post_train:\n",
    "    x_train.append(vocab_mbti.words_to_ids(post.split()))\n",
    "\n",
    "x_test = []\n",
    "for post in post_test:\n",
    "    x_test.append(vocab_mbti.words_to_ids(post.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  i don't agree with brutal..\n",
      "Canonicalized Text:  [308, 2, 361, 187384, 2]\n",
      "Max lengths of texts:  66\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text: \",post_train[88])\n",
    "print(\"Canonicalized Text: \", x_train[88])\n",
    "print(\"Max lengths of texts: \", max([len(x) for x in x_train+x_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#create integer classifiers as 1 hot\n",
    "type_mbti16 = np.array(short_posts[\"type\"])\n",
    "keys = list(set(type_mbti16))\n",
    "values = list(range(len(keys)))\n",
    "label_map = dict(zip(keys, values))\n",
    "\n",
    "y_train = np.array([label_map[label] for label in label_train])\n",
    "one_hot_train = np.zeros((len(label_train),16),dtype=int)\n",
    "one_hot_train[np.arange(len(label_train)), y_train] = 1\n",
    "y_train = one_hot_train\n",
    "\n",
    "y_test = np.array([label_map[label] for label in label_test])\n",
    "one_hot_test = np.zeros((len(label_test),16),dtype=int)\n",
    "one_hot_test[np.arange(len(label_test)), y_test] = 1\n",
    "y_test = one_hot_test\n",
    "\n",
    "print(y_train[:2])\n",
    "print(y_test[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulid the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "V = vocab_mbti.size\n",
    "classes = 16 #len(set(labels))\n",
    "batch_size = 500\n",
    "text_length = 100 # fixed max length of text, but need to handle truncating\n",
    "embed_dim = 300 #256\n",
    "hidden_dims = 300 # Needs to be the same dims for LSTM [256,256]\n",
    "num_layers = 2\n",
    "dropout_keep_prob = .5\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = .001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful functions for building LSTM\n",
    "def MakeFancyRNNCell(hidden_dims, keep_prob, layers=1):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "\n",
    "    Args:\n",
    "      H: hidden state sizes, provided in array\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for _ in range(layers):\n",
    "#         cell = tf.nn.rnn_cell.BasicLSTMCell(H, forget_bias=0.0) #deprecated?\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_dims, forget_bias=0.0,state_is_tuple=True)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, \n",
    "                                             input_keep_prob=keep_prob, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "        cells.append(cell)\n",
    "    return tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "\n",
    "def matmul3d(X, W):\n",
    "    \"\"\"Wrapper for tf.matmul to handle a 3D input tensor X.\n",
    "    Will perform multiplication along the last dimension.\n",
    "\n",
    "    Args:\n",
    "      X: [m,n,k]\n",
    "      W: [k,l]\n",
    "\n",
    "    Returns:\n",
    "      XW: [m,n,l]\n",
    "    \"\"\"\n",
    "    Xr = tf.reshape(X, [-1, tf.shape(X)[2]])\n",
    "    XWr = tf.matmul(Xr, W)\n",
    "    newshape = [tf.shape(X)[0], tf.shape(X)[1], tf.shape(W)[1]]\n",
    "    return tf.reshape(XWr, newshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Input:  (?, ?, 300)\n",
      "LSTM Cell output shape:  (?, ?, 300)\n",
      "flattened output shape:  (?, 30000)\n",
      "W_out:  (30000, 16)\n",
      "Logits:  (?, 16)\n",
      "Pred Prob Shape:  (?, 16)\n",
      "Pred Max Shape:  (?,)\n",
      "Sampling Shape:  (?, 1)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(88)\n",
    "    \n",
    "# Create input placeholder\n",
    "with tf.name_scope(\"Inputs\"):\n",
    "    x_text_ = tf.placeholder(tf.int32, [None, None], name=\"x_text\") #batch x text_length\n",
    "    y_type_ = tf.placeholder(tf.int32, [None,classes], name=\"y_type\") #batch x classes\n",
    "\n",
    "    \n",
    "# Model params\n",
    "with tf.name_scope(\"Dynamic_Params\"):\n",
    "    # Get dynamic shape info from inputs\n",
    "    batch_size_ = tf.shape(x_text_)[0]\n",
    "    ns_ = tf.tile([text_length], [batch_size_, ], name=\"ns\")\n",
    "    \n",
    "# Construct embedding layer\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "#     W_in_ = tf.get_variable(tf.random_uniform(-1.0, 1.0),shape=[V, embed_dim], name=\"W_in\")\n",
    "    W_in_ = tf.Variable(tf.random_uniform([V, embed_dim], -1.0, 1.0), name=\"W_in\")\n",
    "    x_ = tf.nn.embedding_lookup(W_in_, x_text_,name='x')\n",
    "    print(\"Embedded Input: \", x_.shape)\n",
    "\n",
    "# Construct RNN/LSTM cell and recurrent layer.\n",
    "with tf.name_scope(\"Recurrent_Layer\"):\n",
    "    # 2 Layer LSTM\n",
    "    cell_lstm2_ = MakeFancyRNNCell(hidden_dims, dropout_keep_prob, num_layers)\n",
    "    initial_h_ = cell_lstm2_.zero_state(batch_size_, dtype=tf.float32)\n",
    "    output_, final_h_= tf.nn.dynamic_rnn(cell=cell_lstm2_, inputs=x_, \n",
    "                                         sequence_length= ns_, initial_state = initial_h_, dtype=tf.float32)\n",
    "    print(\"LSTM Cell output shape: \",output_.shape)\n",
    "\n",
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    output_ = tf.reshape(output_, [batch_size_,text_length*hidden_dims])\n",
    "    print(\"flattened output shape: \",output_.shape)\n",
    "    W_out_ = tf.Variable(tf.random_uniform([hidden_dims*text_length,classes],-1.0, 1.0), name=\"W_out\")\n",
    "    print(\"W_out: \",W_out_.shape)\n",
    "    b_out_ = tf.Variable(tf.zeros([classes,], dtype=tf.float32), name=\"b_out\")\n",
    "    logits_ = tf.add(tf.matmul(output_,W_out_), b_out_, name=\"logits\")\n",
    "    print(\"Logits: \",logits_.shape)\n",
    "\n",
    "with tf.name_scope(\"Cost_Function\"):\n",
    "    # Full softmax loss for training / scoring\n",
    "    per_example_loss_ = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_type_, \n",
    "                                                                   logits=logits_,\n",
    "                                                                   name=\"per_example_loss\")\n",
    "    loss_ = tf.reduce_mean(per_example_loss_, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"Prediction\"):\n",
    "    pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "    pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "    pred_random_ = tf.reshape(tf.multinomial(tf.reshape(logits_ , [-1, classes]), \n",
    "                                                          1, \n",
    "                                                          output_dtype=tf.int32, \n",
    "                                                          name=\"pred_random\"),\n",
    "                                           [batch_size_,1])\n",
    "    print(\"Pred Prob Shape: \",pred_proba_.shape)\n",
    "    print(\"Pred Max Shape: \",pred_max_.shape)\n",
    "    print(\"Sampling Shape: \",pred_samples_.shape)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"Train\"):\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    optimizer_ = tf.train.AdamOptimizer(learning_rate_)\n",
    "    gradients, variables = zip(*optimizer_.compute_gradients(loss_))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "    train_step_ = optimizer_.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "# Initializer step\n",
    "init_ = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir=\"tf_graph\" --port 6006\n",
    "summary_writer = tf.summary.FileWriter(\"tf_graph\", \n",
    "                                       tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create batched arrays of size batch_size based on input x and input y\n",
    "def pad_np_array(example_ids, max_len=35, pad_id=0):\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns\n",
    "\n",
    "def batch_generator(x, y, batch_size):\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        # padd the batch\n",
    "        x_batch = x[i:i+batch_size]\n",
    "        x_padded, _ = pad_np_array(x_batch, max_len=text_length, pad_id=0)\n",
    "        yield (x_padded, y[i:i+batch_size]) # returns tuple of batched x, batched y\n",
    "\n",
    "def train_batch(session, x, y, learning_rate):\n",
    "    feed_dict = {x_text_:x, #np array of texts\n",
    "                 y_type_:y, #np array of types\n",
    "                 learning_rate_:learning_rate}\n",
    "    c, _ = session.run([loss_, train_step_],\n",
    "                       feed_dict=feed_dict)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 1] seen 0 batches\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-502-22825e532add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[epoch %d] seen %d batches\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mepoch_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtotal_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-499-b4639f42cb8e>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(session, x, y, learning_rate)\u001b[0m\n\u001b[1;32m     21\u001b[0m                  learning_rate_:learning_rate}\n\u001b[1;32m     22\u001b[0m     c, _ = session.run([loss_, train_step_],\n\u001b[0;32m---> 23\u001b[0;31m                        feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_interval = 1000\n",
    "\n",
    "np.random.seed(88)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_)\n",
    "\n",
    "#Override \n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    epoch_cost = 0.0\n",
    "    total_batches = 0\n",
    "    print (\"\")\n",
    "    for i, (x,y) in enumerate(batch_generator(x_train[:100], y_train[:100], batch_size)):\n",
    "        if (i % print_interval == 0):\n",
    "            print(\"[epoch %d] seen %d batches\" % (epoch, i))\n",
    "            \n",
    "        epoch_cost += train_batch(session, x, y, learning_rate)\n",
    "        total_batches = i + 1\n",
    "\n",
    "    avg_cost = epoch_cost / total_batches\n",
    "    print(\"[epoch %d] Completed %d batches in %s\" % (epoch, i, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    print(\"[epoch %d] Average cost: %.03f\" % (epoch, avg_cost,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Inputs/x_text:0\", shape=(?, ?), dtype=int32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1075\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1076\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3589\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3668\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3669\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"Inputs/x_text:0\", shape=(?, ?), dtype=int32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-490-a245863969b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand\u001b[0m \u001b[0;31m# batch x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-490-a245863969b4>\u001b[0m in \u001b[0;36mpredict_type\u001b[0;34m(session, x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx_text_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#np array of texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_max_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand\u001b[0m \u001b[0;31m# batch x 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1076\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1078\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Inputs/x_text:0\", shape=(?, ?), dtype=int32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "def predict_type(session,x):\n",
    "    feed_dict = {x_text_:x} #np array of texts\n",
    "    pred = session.run([pred_max_],feed_dict=feed_dict)\n",
    "    return pred, rand # batch x 1\n",
    "\n",
    "test_pred = predict_type(session,x_train[:5])\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy on train set\n",
    "for i, (x,y) in enumerate(batch_generator(x_train, y_train, batch_size)):\n",
    "    batch_pred = predict_type(session,x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuray on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_batch(session, x, y):\n",
    "    feed_dict = {x_text_:x,\n",
    "                 y_type_:y}\n",
    "    return session.run(loss_, feed_dict=feed_dict)\n",
    "\n",
    "def score_dataset(x, y):\n",
    "    total_cost = 0.0\n",
    "    total_batches = 0\n",
    "    for (x,y) in batch_generator(x, y, 5):\n",
    "        total_cost += score_batch(session, x, y)\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set perplexity: 1299473.379\n",
      "Test set perplexity: 13015846.106\n"
     ]
    }
   ],
   "source": [
    "print (\"Train set perplexity: %.03f\" % np.exp(score_dataset(x_train,y_train)))\n",
    "print (\"Test set perplexity: %.03f\" % np.exp(score_dataset(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
