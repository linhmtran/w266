{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LSTM MBTI Classification Model\n",
    "\n",
    "First, load libraries and useful functions from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "# import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from w266_common import utils, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeFancyRNNCell(hidden_dims, keep_prob):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "\n",
    "    Args:\n",
    "      H: hidden state sizes, provided in array\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for H in hidden_dims:\n",
    "      cell = tf.nn.rnn_cell.BasicLSTMCell(H, forget_bias=0.0)\n",
    "      cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "          cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "      cells.append(cell)\n",
    "    return tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "def matmul3d(X, W):\n",
    "    \"\"\"Wrapper for tf.matmul to handle a 3D input tensor X.\n",
    "    Will perform multiplication along the last dimension.\n",
    "\n",
    "    Args:\n",
    "      X: [m,n,k]\n",
    "      W: [k,l]\n",
    "\n",
    "    Returns:\n",
    "      XW: [m,n,l]\n",
    "    \"\"\"\n",
    "    Xr = tf.reshape(X, [-1, tf.shape(X)[2]])\n",
    "    XWr = tf.matmul(Xr, W)\n",
    "    newshape = [tf.shape(X)[0], tf.shape(X)[1], tf.shape(W)[1]]\n",
    "    return tf.reshape(XWr, newshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Specifications for Baseline LSTM for MBTI\n",
    "\n",
    "In this baseline, the task is to predict the MBTI type (16 types) given a text string. We will model after the A3 assignment, with Architecture and Parameters defined below.\n",
    "\n",
    "### Pre-Processing:\n",
    "* Minimial pre-processing, only separating punctuation from text and lower-case all text\n",
    "* Assigning words to numerical indices based on a fixed Vocab size, defined by word fre-quency in training set\n",
    "\n",
    "### Architecture:\n",
    "* Encoder: 2 layer LSTM\n",
    "* Decoder: Softmax\n",
    "* Classification: 16 MBTI types\n",
    "\n",
    "### Parameters\n",
    "* Batch Size: 500\n",
    "* Text length: dynamic\n",
    "* Vocabulary size (V): |V| - full corpus\n",
    "* Embedding Size: [128,200,256,523]\n",
    "* Hidden Dimensions: [256,300]\n",
    "\n",
    "### Training:\n",
    "* Epochs = 25\n",
    "* 80% train, 20% test\n",
    "* Loss: Cross Entropy with Adam initialization\n",
    "* Optimizers: Adam Optimizer\n",
    "\n",
    "\n",
    "![Generic RNN Architecture for MBTI]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus & Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('./mbti_1.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean and tokenize sentence [\"Hello world.\"] into list of words [\"hello world\"]\n",
    "def clean_tokenize(sentence):\n",
    "    ignore_words = []\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() #nltk.word_tokenize(sentence)\n",
    "#     words = sentence.split() #nltk.word_tokenize(sentence)\n",
    "    words_cleaned = [w.lower() for w in words if w not in ignore_words]\n",
    "    words_cleaned = sentence.lower()\n",
    "    return words_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(422845, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'http://www.youtube.com/watch?v=qsxhcwe3krw</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://41.media.tumblr.com/tumblr_lfouy03pma1q...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what has been the most life-changing experienc...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.youtube.com/watch?v=vxzeywwrdw8   h...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  type  user\n",
       "0        'http://www.youtube.com/watch?v=qsxhcwe3krw  INFJ     0\n",
       "1  http://41.media.tumblr.com/tumblr_lfouy03pma1q...  INFJ     0\n",
       "2  enfp and intj moments  https://www.youtube.com...  INFJ     0\n",
       "3  what has been the most life-changing experienc...  INFJ     0\n",
       "4  http://www.youtube.com/watch?v=vxzeywwrdw8   h...  INFJ     0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split posts per users into separate sentences\n",
    "post = []\n",
    "utype = []\n",
    "user = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    posts = row['posts'].split('|||')\n",
    "    posts_clean = []\n",
    "    for sentence in posts:\n",
    "        posts_clean.append(clean_tokenize(sentence))\n",
    "    post.extend(posts_clean)\n",
    "#     post.extend(posts)\n",
    "    utype.extend([row['type'] for i in range(len(posts))])\n",
    "    user.extend([index for i in range(len(posts))])\n",
    "    \n",
    "short_posts = pd.DataFrame({\"user\": user,\"type\": utype,\"post\": post})\n",
    "print(short_posts.shape)\n",
    "short_posts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBIT posts ['https://www.youtube.com/watch?v=bxvkaah2d7m'\n",
      " 'isfjs and infps can balance each other really well, i think, if they learn to communicate - sjs choosing words (and tone of voice!) carefully, and nps learning to take things less personally. your sj...'\n",
      " \"i'd seek recognition. not fame.\"\n",
      " \"to be honest, maybe you are giving out vibes that you are not self-assured. because if you think about it, a bully is going to go for someone they don't think will fight back, someone who is weak (i...\"\n",
      " \"probably. any thinking, really. personally, i prefer the ax-b-c-dy function stack compared to grant's. it allows for the parts where grant gets it right while still being consistent with jung, i.e.,...\"]\n",
      "\n",
      "MBTI Labels:  ['INTP' 'INFP' 'INTP' 'ENFP' 'INTJ']\n"
     ]
    }
   ],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "post_train, post_test, label_train, label_test = train_test_split(np.array(short_posts['post']), \n",
    "                                                    np.array(short_posts['type']), \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=88)\n",
    "\n",
    "print(\"MBIT posts\", post_train[:5])\n",
    "print('')\n",
    "print(\"MBTI Labels: \",label_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333007"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary (V size is defaulted to full text)\n",
    "# vocab_mbti = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in utils.flatten(short_posts['post'])))\n",
    "vocab_mbti = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in post_train))\n",
    "vocab_mbti.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202, 147565, 317206, 159348]\n",
      "['a', 'what', 'and', 'the']\n"
     ]
    }
   ],
   "source": [
    "print (vocab_mbti.words_to_ids(['a','what','and','the']))\n",
    "print (vocab_mbti.ids_to_words([202, 147565, 317206, 159348])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# canonicalize train and test sets\n",
    "x_train = []\n",
    "for post in post_train:\n",
    "    x_train.append(vocab_mbti.words_to_ids(post.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i don't agree with brutal..\n",
      "[308, 2, 361, 187384, 2]\n"
     ]
    }
   ],
   "source": [
    "print(post_train[88])\n",
    "print(x_train[88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ISFJ': 1, 'ESFP': 2, 'INFJ': 3, 'ENTJ': 4, 'ENFJ': 5, 'ISFP': 6, 'ESTJ': 7, 'INTP': 8, 'ESTP': 9, 'ESFJ': 10, 'INFP': 11, 'ISTP': 12, 'ENTP': 13, 'INTJ': 14, 'ENFP': 15, 'ISTJ': 16}\n",
      "[ 8 11  8 15 14]\n"
     ]
    }
   ],
   "source": [
    "#create integer classifiers as 1 hot\n",
    "type_mbti16 = np.array(short_posts[\"type\"])\n",
    "keys = list(set(type_mbti16))\n",
    "values = list(range(1,len(keys)+1))\n",
    "label_map = dict(zip(keys, values))\n",
    "print(label_map)\n",
    "\n",
    "y_train = np.array([label_map[label] for label in label_train])\n",
    "y_test = np.array([label_map[label] for label in label_test])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulid the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "V = vocab_mbti.size\n",
    "classes = 16 #len(set(labels))\n",
    "batch_size = 500 # this will be used for creating the batched sets (changed from 500 to mini size 10)\n",
    "embed_dim = 256\n",
    "hidden_dims = [256,300]\n",
    "dropout_keep_prob = .5\n",
    "\n",
    "# Training Parameters\n",
    "softmax_ns = 5 # probably don't need this for 16 classes\n",
    "learning_rate = .001\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(88)\n",
    "\n",
    "# Create input placeholder\n",
    "with tf.name_scope(\"Inputs\"):\n",
    "    x_text_ = tf.placeholder(tf.int32, [None, None], name=\"x_text\") #batch x text_length\n",
    "    y_type_ = tf.placeholder(tf.int32, [None,classes], name=\"y_type\") #batch x 1\n",
    "    # Get dynamic shape info from inputs\n",
    "    batch_size_ = tf.shape(x_text_)[0]\n",
    "    text_length_ = tf.shape(x_text_)[1]\n",
    "    ns_ = tf.tile([text_length_], [batch_size_, ], name=\"ns\")\n",
    "    \n",
    "# Construct embedding layer\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    W_in_ = tf.Variable(tf.random_uniform([V, embed_dim], -1.0, 1.0), name=\"W_in\")\n",
    "    x_ = tf.nn.embedding_lookup(W_in_, x_text_)\n",
    "\n",
    "# Construct RNN/LSTM cell and recurrent layer.\n",
    "with tf.name_scope(\"Recurrent_Layer\"):\n",
    "    cell_ = MakeFancyRNNCell(hidden_dims, dropout_keep_prob)\n",
    "    initial_h_ = cell_.zero_state(batch_size_, dtype=tf.float32)\n",
    "    output_, final_h_= tf.nn.dynamic_rnn(cell_, x_, sequence_length= ns_,dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    W_out_ = tf.Variable(tf.random_uniform([hidden_dims[-1],classes],-1.0, 1.0), name=\"W_out\")\n",
    "    b_out_ = tf.Variable(tf.zeros([classes,], dtype=tf.float32), name=\"b_out\")\n",
    "    logits_ = tf.add(matmul3d(output_, W_out_), b_out_, name=\"logits\")\n",
    "\n",
    "with tf.name_scope(\"Prediction\"):\n",
    "    pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "    pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "    pred_samples_ = tf.reshape(tf.multinomial(tf.reshape(logits_ , [-1, classes]), \n",
    "                                                          1, \n",
    "                                                          output_dtype=tf.int32, \n",
    "                                                          name=\"pred_samples\"),\n",
    "                                           [batch_size_,1])\n",
    "with tf.name_scope(\"Cost_Function\"):\n",
    "    # Sampled Softmax loss for training. Do we need this for only 16 classes?\n",
    "#     train_inputs_ = tf.reshape(output_, [batch_size*text_length,-1])\n",
    "#     per_example_train_loss_ = tf.nn.sampled_softmax_loss(weights=tf.transpose(W_out_),\n",
    "#                                                          biases=b_out_,\n",
    "#                                                          labels=tf.reshape(target_y_, [-1, 1]), \n",
    "#                                                          inputs=tf.reshape(output_, [batch_size*text_length,-1]),\n",
    "#                                                          num_sampled=softmax_ns, \n",
    "#                                                          num_classes=classes,\n",
    "#                                                          name=\"per_example_sampled_softmax_loss\")\n",
    "\n",
    "#     train_loss_ = tf.reduce_mean(per_example_train_loss_, name=\"sampled_softmax_loss\")\n",
    "    # Full softmax loss for scoriing\n",
    "#     loss_ = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_type_,\n",
    "#                                                                            logits=logits_,\n",
    "#                                                                            name=\"loss\"))\n",
    "    per_example_loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_type_, \n",
    "                                                                       logits=logits_, \n",
    "                                                                       name=\"per_example_loss\")\n",
    "    loss_ = tf.reduce_mean(per_example_loss_, name=\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Train\"):\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    optimizer_ = tf.train.AdamOptimizer(learning_rate)\n",
    "#     gradients, variables = zip(*optimizer_.compute_gradients(train_loss_))\n",
    "    gradients, variables = zip(*optimizer_.compute_gradients(loss_))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "    train_step_ = optimizer_.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "# Initializer step\n",
    "init_ = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir=\"tf_graph\" --port 6006\n",
    "summary_writer = tf.summary.FileWriter(\"tf_graph\", \n",
    "                                       tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create batched arrays of size batch_size based on input x and input y\n",
    "def pad_np_array(example_ids, max_len=35, pad_id=0):\n",
    "    \"\"\"Pad a list of lists of ids into a rectangular NumPy array.\n",
    "    Longer sequences will be truncated to max_len ids, while shorter ones will\n",
    "    be padded with pad_id.\n",
    "    Args:\n",
    "        example_ids: list(list(int)), sequence of ids for each example\n",
    "        max_len: maximum sequence length\n",
    "        pad_id: id to pad shorter sequences with\n",
    "    Returns: (x, ns)\n",
    "        x: [num_examples, max_len] NumPy array of integer ids\n",
    "        ns: [num_examples] NumPy array of sequence lengths (<= max_len)\n",
    "    \"\"\"\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns\n",
    "\n",
    "def batch_generator(x, y, batch_size):\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        # padd the batch\n",
    "        x_batch = x[i:i+batch_size]\n",
    "        max_len_batch = max([len(x) for x in x_batch])\n",
    "        x_padded, _ = pad_np_array(x_batch, max_len=max_len_batch, pad_id=0)\n",
    "        yield (x_padded, y[i:i+batch_size]) # returns tuple of batched x, batched y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(session, x, y, learning_rate):\n",
    "    feed_dict = {x_text_:x, #np array of texts\n",
    "                 y_type_:y, #np array of types\n",
    "                 learning_rate_:learning_rate}\n",
    "    print(feed_dict)\n",
    "    c, _ = session.run([loss_, train_step_],\n",
    "                       feed_dict=feed_dict)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 1] seen 0 batches\n",
      "{<tf.Tensor 'Inputs/x_text:0' shape=(?, ?) dtype=int32>: array([[  1492,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0],\n",
      "       [     2, 317206,      2,      2, 258702,      2,      2,      2,\n",
      "             2,    308,      2,      2,      2,      2,      2,      2,\n",
      "            42,      2,      2,    758,      2,      2,      2,      2,\n",
      "             2, 317206,      2,      2,      2,      2,      2,      2,\n",
      "             2,      2,      2]], dtype=int32), <tf.Tensor 'Inputs/y_type:0' shape=(?, ?) dtype=int32>: array([ 8, 11]), <tf.Tensor 'Train_1/learning_rate:0' shape=<unknown> dtype=float32>: 0.001}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (2,) for Tensor 'Inputs/y_type:0', which has shape '(?, ?)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-a85d718aae69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[epoch %d] seen %d batches\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mepoch_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-15f54950169d>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(session, x, y, learning_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     c, _ = session.run([loss_, train_step_],\n\u001b[0;32m----> 7\u001b[0;31m                        feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/linhtran/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1111\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1112\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (2,) for Tensor 'Inputs/y_type:0', which has shape '(?, ?)'"
     ]
    }
   ],
   "source": [
    "print_interval = 1000\n",
    "\n",
    "np.random.seed(88)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_)\n",
    "\n",
    "# for testing\n",
    "batch_size = 2\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    epoch_cost = 0.0\n",
    "    total_batches = 0\n",
    "    print (\"\")\n",
    "    for i, (x,y) in enumerate(batch_generator(x_train, y_train, batch_size)):\n",
    "        if (i % print_interval == 0):\n",
    "            print(\"[epoch %d] seen %d batches\" % (epoch, i))\n",
    "        \n",
    "        epoch_cost += train_batch(session, x, y, learning_rate)\n",
    "        break\n",
    "        total_batches = i + 1\n",
    "\n",
    "    avg_cost = epoch_cost / total_batches\n",
    "    print(\"[epoch %d] Completed %d batches in %s\" % (epoch, i, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    print(\"[epoch %d] Average cost: %.03f\" % (epoch, avg_cost,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_batch(session, x, y):\n",
    "    feed_dict = {x_text_:x,\n",
    "                 y_type_:y}\n",
    "    return session.run(loss_, feed_dict=feed_dict)\n",
    "\n",
    "def score_dataset(x, y):\n",
    "    total_cost = 0.0\n",
    "    total_batches = 0\n",
    "    for (x,y) in batch_generator(x, y, 1000):\n",
    "        total_cost += score_batch(session, batch)\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Train set perplexity: %.03f\" % np.exp(score_dataset(x_train,y_train))\n",
    "print \"Test set perplexity: %.03f\" % np.exp(score_dataset(x_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
