{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline LSTM MBTI Classification Model\n",
    "\n",
    "First, load libraries and useful functions from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Utils and Helper libraries\n",
    "# import nltk\n",
    "from w266_common import utils, vocabulary\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Specifications for Baseline LSTM for MBTI\n",
    "\n",
    "In this baseline, the task is to predict the MBTI type (16 types) given a text string. We will model after the A3 assignment, with Architecture and Parameters defined below.\n",
    "\n",
    "### Pre-Processing:\n",
    "* Minimial pre-processing, only separating punctuation from text and lower-case all text\n",
    "* Assigning words to numerical indices based on a fixed Vocab size, defined by word fre-quency in training set\n",
    "\n",
    "### Architecture:\n",
    "* Encoder: 1 layer LSTM\n",
    "* Decoder: Affine on the LSTM output with Softmax on logits\n",
    "* Classification: 16 MBTI types\n",
    "\n",
    "### Parameters\n",
    "* Batch Size: 500 (50 for pilot)\n",
    "* Text length: 100\n",
    "* Vocabulary size (V): ~330K - full corpus\n",
    "* Embedding Size: 300\n",
    "* Hidden Dimensions: 300\n",
    "\n",
    "### Training:\n",
    "* Epochs = 25 (5 for pilot)\n",
    "* 80% train, 20% test\n",
    "* Loss: Cross Entropy with Adam initialization\n",
    "* Optimizers: Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus & Pre-Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('./mbti_1.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean and tokenize sentence [\"Hello world.\"] into list of words [\"hello world\"]\n",
    "def clean(sentence):\n",
    "    ignore_words = []\n",
    "    words = re.sub(\"[^\\w]\", \" \",  sentence).split() #nltk.word_tokenize(sentence)\n",
    "#     words = sentence.split() #nltk.word_tokenize(sentence)\n",
    "    words_cleaned = sentence.lower()\n",
    "    return words_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(422845, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>type</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'http://www.youtube.com/watch?v=qsxhcwe3krw</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://41.media.tumblr.com/tumblr_lfouy03pma1q...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enfp and intj moments  https://www.youtube.com...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what has been the most life-changing experienc...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.youtube.com/watch?v=vxzeywwrdw8   h...</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  type  user\n",
       "0        'http://www.youtube.com/watch?v=qsxhcwe3krw  INFJ     0\n",
       "1  http://41.media.tumblr.com/tumblr_lfouy03pma1q...  INFJ     0\n",
       "2  enfp and intj moments  https://www.youtube.com...  INFJ     0\n",
       "3  what has been the most life-changing experienc...  INFJ     0\n",
       "4  http://www.youtube.com/watch?v=vxzeywwrdw8   h...  INFJ     0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split posts per users into separate sentences\n",
    "post = []\n",
    "utype = []\n",
    "user = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    posts = row['posts'].split('|||')\n",
    "    posts_clean = []\n",
    "    for sentence in posts:\n",
    "        posts_clean.append(clean(sentence))\n",
    "    post.extend(posts_clean)\n",
    "#     post.extend(posts)\n",
    "    utype.extend([row['type'] for i in range(len(posts))])\n",
    "    user.extend([index for i in range(len(posts))])\n",
    "    \n",
    "short_posts = pd.DataFrame({\"user\": user,\"type\": utype,\"post\": post})\n",
    "print(short_posts.shape)\n",
    "short_posts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBIT posts ['https://www.youtube.com/watch?v=bxvkaah2d7m'\n",
      " 'isfjs and infps can balance each other really well, i think, if they learn to communicate - sjs choosing words (and tone of voice!) carefully, and nps learning to take things less personally. your sj...'\n",
      " \"i'd seek recognition. not fame.\"\n",
      " \"to be honest, maybe you are giving out vibes that you are not self-assured. because if you think about it, a bully is going to go for someone they don't think will fight back, someone who is weak (i...\"\n",
      " \"probably. any thinking, really. personally, i prefer the ax-b-c-dy function stack compared to grant's. it allows for the parts where grant gets it right while still being consistent with jung, i.e.,...\"]\n",
      "\n",
      "MBTI Labels:  ['INTP' 'INFP' 'INTP' 'ENFP' 'INTJ']\n"
     ]
    }
   ],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "post_train, post_test, label_train, label_test = train_test_split(np.array(short_posts['post']), \n",
    "                                                    np.array(short_posts['type']), \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=88)\n",
    "\n",
    "print(\"MBIT posts\", post_train[:5])\n",
    "print('')\n",
    "print(\"MBTI Labels: \",label_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333007"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vocabulary (V size is defaulted to full text) for train corpus\n",
    "vocab_mbti = vocabulary.Vocabulary((utils.canonicalize_word(w) for w in post_train))\n",
    "vocab_mbti.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202, 147565, 317206, 159348]\n",
      "['a', 'what', 'and', 'the']\n"
     ]
    }
   ],
   "source": [
    "print (vocab_mbti.words_to_ids(['a','what','and','the']))\n",
    "print (vocab_mbti.ids_to_words([202, 147565, 317206, 159348])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize and canonicalize train and test sets\n",
    "x_train = []\n",
    "for post in post_train:\n",
    "    x_train.append(vocab_mbti.words_to_ids(post.split()))\n",
    "\n",
    "x_test = []\n",
    "for post in post_test:\n",
    "    x_test.append(vocab_mbti.words_to_ids(post.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  i don't agree with brutal..\n",
      "Canonicalized Text:  [308, 2, 361, 187384, 2]\n",
      "Max lengths of texts:  66\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text: \",post_train[88])\n",
    "print(\"Canonicalized Text: \", x_train[88])\n",
    "print(\"Max lengths of texts: \", max([len(x) for x in x_train+x_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3]\n",
      "[3 8]\n",
      "[[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#create integer classifiers as 1 hot\n",
    "type_mbti16 = np.array(short_posts[\"type\"])\n",
    "keys = list(set(type_mbti16))\n",
    "values = list(range(len(keys)))\n",
    "label_map = dict(zip(keys, values))\n",
    "\n",
    "y_train_id = np.array([label_map[label] for label in label_train])\n",
    "one_hot_train = np.zeros((len(label_train),16),dtype=int)\n",
    "one_hot_train[np.arange(len(label_train)), y_train_id] = 1\n",
    "y_train = one_hot_train\n",
    "\n",
    "y_test_id = np.array([label_map[label] for label in label_test])\n",
    "one_hot_test = np.zeros((len(label_test),16),dtype=int)\n",
    "one_hot_test[np.arange(len(label_test)), y_test_id] = 1\n",
    "y_test = one_hot_test\n",
    "\n",
    "print(y_train_id[:2])\n",
    "print(y_test_id[:2])\n",
    "print(y_train[:2])\n",
    "print(y_test[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulid the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "V = vocab_mbti.size\n",
    "classes = 16 #len(set(labels))\n",
    "batch_size = 500\n",
    "text_length = 100 # fixed max length of text, but need to handle truncating\n",
    "embed_dim = 300 #256\n",
    "hidden_dims = 300 # Needs to be the same dims for LSTM [256,256]\n",
    "num_layers = 1\n",
    "\n",
    "# Training Parameters\n",
    "dropout_keep_prob = .5\n",
    "learning_rate = .001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# useful functions for building LSTM\n",
    "def MakeFancyRNNCell(hidden_dims, keep_prob, layers=1, isTrain=True):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "\n",
    "    Args:\n",
    "      H: hidden state sizes, provided in array\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    if isTrain == False:\n",
    "        keep_prob = 1.0\n",
    "\n",
    "    cells = []\n",
    "    for _ in range(layers):\n",
    "#         cell = tf.nn.rnn_cell.BasicLSTMCell(H, forget_bias=0.0) #deprecated?\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_dims, forget_bias=0.0,state_is_tuple=True)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, \n",
    "                                             input_keep_prob=keep_prob, \n",
    "                                             output_keep_prob=keep_prob)\n",
    "        cells.append(cell)\n",
    "    return tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "\n",
    "# def matmul3d(X, W):\n",
    "#     \"\"\"Wrapper for tf.matmul to handle a 3D input tensor X.\n",
    "#     Will perform multiplication along the last dimension.\n",
    "\n",
    "#     Args:\n",
    "#       X: [m,n,k]\n",
    "#       W: [k,l]\n",
    "\n",
    "#     Returns:\n",
    "#       XW: [m,n,l]\n",
    "#     \"\"\"\n",
    "#     Xr = tf.reshape(X, [-1, tf.shape(X)[2]])\n",
    "#     XWr = tf.matmul(Xr, W)\n",
    "#     newshape = [tf.shape(X)[0], tf.shape(X)[1], tf.shape(W)[1]]\n",
    "#     return tf.reshape(XWr, newshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Input:  (?, ?, 300)\n",
      "LSTM Cell output shape:  (?, ?, 300)\n",
      "flattened output shape:  (?, 30000)\n",
      "W_out:  (30000, 16)\n",
      "Logits:  (?, 16)\n",
      "Pred Prob Shape:  (?, 16)\n",
      "Pred Max Shape:  (?,)\n",
      "Sampling Shape:  (?, 1)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(88)\n",
    "    \n",
    "# Create input placeholder\n",
    "with tf.name_scope(\"Inputs\"):\n",
    "    x_text_ = tf.placeholder(tf.int32, [None, None], name=\"x_text\") #batch x text_length\n",
    "    y_type_ = tf.placeholder(tf.int32, [None,classes], name=\"y_type\") #batch x classes\n",
    "\n",
    "    \n",
    "# Model params\n",
    "with tf.name_scope(\"Dynamic_Params\"):\n",
    "    # Get dynamic shape info from inputs\n",
    "    batch_size_ = tf.shape(x_text_)[0]\n",
    "    ns_ = tf.tile([text_length], [batch_size_, ], name=\"ns\")\n",
    "    isTrain_ = tf.placeholder(tf.bool, shape=())\n",
    "\n",
    "# Construct embedding layer\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "#     W_in_ = tf.get_variable(tf.random_uniform(-1.0, 1.0),shape=[V, embed_dim], name=\"W_in\")\n",
    "    W_in_ = tf.Variable(tf.random_uniform([V, embed_dim], -1.0, 1.0), name=\"W_in\")\n",
    "    x_ = tf.nn.embedding_lookup(W_in_, x_text_,name='x')\n",
    "    print(\"Embedded Input: \", x_.shape)\n",
    "\n",
    "# Construct RNN/LSTM cell and recurrent layer.\n",
    "with tf.name_scope(\"Recurrent_Layer\"):\n",
    "    # 2 Layer LSTM\n",
    "    cell_lstm2_ = MakeFancyRNNCell(hidden_dims, dropout_keep_prob, num_layers,isTrain_)\n",
    "    initial_h_ = cell_lstm2_.zero_state(batch_size_, dtype=tf.float32)\n",
    "    output_, final_h_= tf.nn.dynamic_rnn(cell=cell_lstm2_, inputs=x_, \n",
    "                                         sequence_length= ns_, initial_state = initial_h_, dtype=tf.float32)\n",
    "    print(\"LSTM Cell output shape: \",output_.shape)\n",
    "\n",
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    output_ = tf.reshape(output_, [batch_size_,text_length*hidden_dims])\n",
    "    print(\"flattened output shape: \",output_.shape)\n",
    "    W_out_ = tf.Variable(tf.random_uniform([hidden_dims*text_length,classes],-1.0, 1.0), name=\"W_out\")\n",
    "    print(\"W_out: \",W_out_.shape)\n",
    "    b_out_ = tf.Variable(tf.zeros([classes,], dtype=tf.float32), name=\"b_out\")\n",
    "    logits_ = tf.add(tf.matmul(output_,W_out_), b_out_, name=\"logits\")\n",
    "    print(\"Logits: \",logits_.shape)\n",
    "\n",
    "with tf.name_scope(\"Cost_Function\"):\n",
    "    # Full softmax loss for training / scoring\n",
    "    per_example_loss_ = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_type_, \n",
    "                                                                   logits=logits_,\n",
    "                                                                   name=\"per_example_loss\")\n",
    "    loss_ = tf.reduce_mean(per_example_loss_, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"Prediction\"):\n",
    "    pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "    pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "    pred_random_ = tf.reshape(tf.multinomial(tf.reshape(logits_ , [-1, classes]), \n",
    "                                                          1, \n",
    "                                                          output_dtype=tf.int32, \n",
    "                                                          name=\"pred_random\"),\n",
    "                                           [batch_size_,1])\n",
    "    print(\"Pred Prob Shape: \",pred_proba_.shape)\n",
    "    print(\"Pred Max Shape: \",pred_max_.shape)\n",
    "    print(\"Sampling Shape: \",pred_random_.shape)\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"Train\"):\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    optimizer_ = tf.train.AdamOptimizer(learning_rate_)\n",
    "    gradients, variables = zip(*optimizer_.compute_gradients(loss_))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "    train_step_ = optimizer_.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "# Initializer step\n",
    "init_ = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorboard --logdir=\"tf_graph\" --port 6006\n",
    "summary_writer = tf.summary.FileWriter(\"tf_graph\", \n",
    "                                       tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create batched arrays of size batch_size based on input x and input y\n",
    "def pad_np_array(example_ids, max_len=35, pad_id=0):\n",
    "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
    "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
    "    for i, ids in enumerate(example_ids):\n",
    "        cpy_len = min(len(ids), max_len)\n",
    "        arr[i,:cpy_len] = ids[:cpy_len]\n",
    "        ns[i] = cpy_len\n",
    "    return arr, ns\n",
    "\n",
    "def batch_generator(x, y, batch_size):\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        # padd the batch\n",
    "        x_batch = x[i:i+batch_size]\n",
    "        x_padded, _ = pad_np_array(x_batch, max_len=text_length, pad_id=0)\n",
    "        yield (x_padded, y[i:i+batch_size]) # returns tuple of batched x, batched y\n",
    "\n",
    "def train_batch(session, xbatch, ybatch, learning_rate):\n",
    "    feed_dict = {x_text_:xbatch, #np array of texts\n",
    "                 y_type_:ybatch, #np array of types\n",
    "                 learning_rate_:learning_rate}\n",
    "    c, _ = session.run([loss_, train_step_],\n",
    "                       feed_dict=feed_dict)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 1] seen 0 batches\n",
      "[epoch 1] Completed 99 batches in 0:02:42\n",
      "[epoch 1] Average cost: 13.083\n",
      "\n",
      "[epoch 2] seen 0 batches\n",
      "[epoch 2] Completed 99 batches in 0:02:37\n",
      "[epoch 2] Average cost: 4.959\n",
      "\n",
      "[epoch 3] seen 0 batches\n",
      "[epoch 3] Completed 99 batches in 0:02:32\n",
      "[epoch 3] Average cost: 3.643\n",
      "\n",
      "[epoch 4] seen 0 batches\n",
      "[epoch 4] Completed 99 batches in 0:02:28\n",
      "[epoch 4] Average cost: 2.965\n",
      "\n",
      "[epoch 5] seen 0 batches\n",
      "[epoch 5] Completed 99 batches in 0:02:31\n",
      "[epoch 5] Average cost: 2.543\n"
     ]
    }
   ],
   "source": [
    "print_interval = 1000\n",
    "\n",
    "np.random.seed(88)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_)\n",
    "\n",
    "#Override for test training\n",
    "batch_size = 50\n",
    "num_epochs = 5\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    epoch_cost = 0.0\n",
    "    total_batches = 0\n",
    "    print (\"\")\n",
    "    for i, (x,y) in enumerate(batch_generator(x_train[:5000], y_train[:5000], batch_size)):\n",
    "        if (i % print_interval == 0):\n",
    "            print(\"[epoch %d] seen %d batches\" % (epoch, i))\n",
    "            \n",
    "        epoch_cost += train_batch(session, x, y, learning_rate)\n",
    "        total_batches = i + 1\n",
    "\n",
    "    avg_cost = epoch_cost / total_batches\n",
    "    print(\"[epoch %d] Completed %d batches in %s\" % (epoch, i, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    print(\"[epoch %d] Average cost: %.03f\" % (epoch, avg_cost,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_type(session,x):\n",
    "    inputs = {x_text_:x,#np array of texts\n",
    "             isTrain_: False} \n",
    "    pred = session.run([pred_max_],feed_dict=inputs)\n",
    "    return pred # batch x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.3502\n"
     ]
    }
   ],
   "source": [
    "# Accuracy on train set\n",
    "train_accuracy=[]\n",
    "for i, (x,y) in enumerate(batch_generator(x_train[:5000], y_train_id[:5000], batch_size)):\n",
    "    pred = predict_type(session,x)\n",
    "    train_accuracy.append((pred[0] == y).mean())\n",
    "print(\"Train accuracy: \" ,np.mean(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.161\n"
     ]
    }
   ],
   "source": [
    "# Accuray on test set\n",
    "test_accuracy = []\n",
    "for i, (x,y) in enumerate(batch_generator(x_test[:1000], y_test_id[:1000], batch_size)):\n",
    "    pred = predict_type(session,x)\n",
    "    test_accuracy.append((pred[0] == y).mean())\n",
    "print(\"Test accuracy: \",np.mean(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_batch(session, x, y):\n",
    "    feed_dict = {x_text_:x,\n",
    "                 y_type_:y}\n",
    "    return session.run(loss_, feed_dict=feed_dict)\n",
    "\n",
    "def score_dataset(x, y):\n",
    "    total_cost = 0.0\n",
    "    total_batches = 0\n",
    "    for (x,y) in batch_generator(x, y, 5):\n",
    "        total_cost += score_batch(session, x, y)\n",
    "        total_batches += 1\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set perplexity: 9.097\n",
      "Test set perplexity: 41.244\n"
     ]
    }
   ],
   "source": [
    "print (\"Train set perplexity: %.03f\" % np.exp(score_dataset(x_train[:5000],y_train[:5000])))\n",
    "print (\"Test set perplexity: %.03f\" % np.exp(score_dataset(x_test[:2000],y_test[:2000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
